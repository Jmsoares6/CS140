			+--------------------+
			| CS 140             |
			| PROJECT 1: THREADS |
			| DESIGN DOCUMENT    |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Song Han <songhan@stanford.edu>
Jinchao Ye <jinchao@stanford.edu>
Bo Wang <bowang@stanford.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

None

>> Describe briefly which parts of the assignment were implemented by
>> each member of your team. If some team members contributed significantly
>> more or less than others (e.g. 2x), indicate that here.

Song Han: alarm, priority scheduler, system merge and debug, design document 
Jinchao Ye: advanced scheduler, design document
Bo Wang: priority donation, design document, testing cases

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---thread.h---
Added two fields to struct thread:
    int64_t wake_up_time;            /* Time to wake up current thread */
    struct list_elem alarm_elem;     /* List element for alarm queue */

---timer.c---
Added an alarm_list which contains all the sleeping threads.
    static struct list alarm_list; /* List of threads waiting for alarm */

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

When a a thread calls timer_sleep() it means that it's going to sleep for a 
while. We first calculate the wake up time, which is given by start+ticks. 
To avoid busy waiting, a thread in sleep should be blocked, and a new
thread should get started to run. The way we keep track of all the sleeping 
thread is through the alarm_list. Since this list is global and shared among
all threads, it's modification should be protected by disbling interrupts. 
One more reason to disable interrupt is that blocking a thread will incur 
schedule(), so when doing context switch we should disable interrupt.

One thing to consider is the sequence in which the sleeping threads are
inserted. Since at each tick we should check if there's a thread that needs
to be waken up, it's better to keep the queue sorted by the wake up time. 
Hence we did list_insert_ordered for each sleeping thread with their wake 
up time from small to large. Thus, the searching complexity will be smaller. 



>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

We kept a sorted alarm_list at timer_sleep(), so in the timer interrupt 
handler we only need to peek the front of the queue to see if a thread 
needs to be waken up. This complexity is k*O(1), where k is the number of
threads that should be waken up at the same time.

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

The race condition is avoided by disabling interrupts when we manipulate 
the alarm list and blocking the current thread. Therefore when multiple 
threads are trying to sleep, only one thread can manipulate the alarm_list 
and block it self. This operation is therefore atomic.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

The timer interrupt handler also manipulates the alarm_list, since it try to
see if the time comes to wake up some thread and get them out of the list. 
However,in both the timer interrupt handler and the timer_sleep, both 
manipulation to the global variable of this alarm_list is protected by 
disabling interrupts. So no race condition could happen. 

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

There are several design considerations in current implementation: first 
the sleeping threads are kept track of through the alarm_list, each thread
is added an additional node, the alarm_elem that link them together. 
Another additional component is the wake_up_time. All these two adds upto 
16 Byte to the structure of thread, since they are small compared to 4KB 
space and they are static allocated, they won't lead to stack over flow. The
other consideration is the sorted alarm_list, which decreased complexity 
during timer interrupt handler. 

One alternative approach, with the motivation of avoid adding more components
in the thread structure, is to make another structure that points to the waking
thread and recording this thread's wake up time at same time. It has an 
additional list_elem component that link them together. 

    struct node{
        struct thread *t;   //pointer to the sleeping thread
        int64_t wake_up_time; //when to wake up
        list_elem element;  //link these nodes together
    }

This approach has the advantage of not occupying additional space to thread
structure. However, each time a thread trys to sleep we have to dynamically 
allocate such a node to track the sleeping node, and when it wake up we have 
to free such a node. This overhead should be taken into consideration. However,
in our approach, we don't need to dynamically allocate a space to trake the 
thread, we just link the thread itself to the alarm_list. Neither do we need 
to free the space when we wake up the thread. 

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---sync.h---
One member is added to struct lock.
    struct list_elem thread_elem; /* List element for waited lock queue */

---thread.h---
Three members are added to struct thread
    int eff_priority;                   /* Effective priority */
    struct lock * lock_to_acquire;      /* Lock this thread is waiting for */
    struct list locks_waited_by_others; /* Locks held by this thread but 
                                           also waited by other threads */

>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

The variable eff_priority denotes the current priority with donation. We use 
this separate variable to avoid overwrite the generic priority of the thread.
When a thread acquires a lock, if not successful, it will add itself to the 
waiting list at lock->semaphore->waiters. Then the acquiring thread will look
for the lock holder thread and donate priority if the acquiring thread's
priority is higher. The effective priority update is done by calling 
thread_set_priority function on the holder thread. This function will check 
whether its lock_to_acquire is null. If not null, it will recursively call 
thread_set_eff_priority on the lock holder it is waiting for.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

When the lock to acquire is not available, a priority donation is triggered.
Thread A: the acquiring thread
Thread B: the holder thread
1) Set lock_to_acquire in Thread A
2) Insert this lock to Thread B's locks_waited_by_others list
3) Insert Thread A into the lock->semaphore->waiters list based on eff_priority
4) If A's priority is higher than that of B, set B's eff_priority by calling
   thread_set_eff_priority on B
5) Block Thread A

In thread_set_eff_priority function, nested donation is handled.
When thread_set_eff_priority(struct thread *t, int eff_priority) is called:
1) Set thread's eff_priority to the new eff_priority
2) If the thread's lock_to_acquire is not NULL, nested donation is triggered
   i)   Since the thread's eff_priority is changed, sort the waiters list of
        lock_to_acquire.
   ii)  If the thread's eff_priority increases, call thread_set_eff_priority
        on the holder thread of lock_to_acquire with the new eff_priority
   iii) If the thread's eff_priority decreases, first search for the highest
        priority among all locks_waited_by_others and the priority of the 
        thread itself; then call thread_set_eff_priority on the holder thread
        of lock_to_acquire with the eff_priority just found

In a nutshell, nested donation is handled by recursive calls to function
thread_set_eff_priority.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu    priority   thread
ticks   A   B   C   A   B   C   to run
-----  --  --  --  --  --  --   ------
 0
 4
 8
12
16
20
24
28
32
36

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
